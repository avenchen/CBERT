{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  24\n"
     ]
    }
   ],
   "source": [
    "import torch# If there's a GPU available...\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" ##I will find a way to fix this later :(\n",
    "\n",
    "NUM_GPUS=0\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():  \n",
    "        device = torch.device(\"cuda\")\n",
    "        NUM_GPUS=torch.cuda.device_count()\n",
    "        print('There are %d GPU(s) available.' % NUM_GPUS)\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name())# If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")  \n",
    "except:\n",
    "    print('Cuda error using CPU instead.')\n",
    "    device = torch.device(\"cpu\")  \n",
    "    \n",
    "print(device)\n",
    "\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "print(\"Cpu count: \",NUM_PROCESSORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading directory:  ./Results\n",
      "Model Saving directory: ./Results/NVD/Model/\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.Dataset import getDataset, getDummyDataset, getRandomDataset, Data        \n",
    "\n",
    "DIR='./Results'\n",
    "    \n",
    "from pathlib import Path\n",
    "Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_LOAD_DIR=\"./NVD/\"\n",
    "MODEL_SAVE_DIR=DIR+'/NVD/Model/'\n",
    "\n",
    "Path(MODEL_SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Data loading directory: \", DIR)\n",
    "print(\"Model Saving directory:\", MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import zipfile\n",
    "import wget\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import RandomSampler,SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss, CosineEmbeddingLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelWithLMHead\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
    "from transformers.models.roberta.modeling_roberta import RobertaLMHead\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.trainer.supporters import CombinedLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For reproduciblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "from random import sample\n",
    "\n",
    "seed_val = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "pl.seed_everything(seed_val)\n",
    "\n",
    "try:\n",
    "    torch.cuda.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except:\n",
    "    print(\"nothing to set for cudnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy precision level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_NEG_LINKS=None\n",
    "TOP_K0=[1,3,5]\n",
    "TOP_K1=[1,2,2]\n",
    "TOP_K2=[1,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results(filename,infos,append=True):    \n",
    "    f=None\n",
    "    if append==True:  \n",
    "        f=open(filename, 'a+')\n",
    "    else:\n",
    "        f=open(filename, 'w')\n",
    "    \n",
    "    f.write(\"\\n\")    \n",
    "    for key, values in infos.items():\n",
    "        f.write(\"%s :\" % key)\n",
    "        if type(values).__name__=='list':            \n",
    "            for item in values:\n",
    "                f.write(\"%s \" % item)\n",
    "        else:\n",
    "            f.write(\"%s \" % values)           \n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.close()\n",
    "#log_results(\"testing\",{'train_acc':[1,2,3,4]})\n",
    "#log_results(\"testing\",{'train_acc':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CVE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, org_labels, collator, data, k_neg_link=10, use_collator=True):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.org_labels = org_labels\n",
    "        self.collator = collator\n",
    "        self.data = data\n",
    "        self.k_neg_link=k_neg_link\n",
    "        self.use_collator=use_collator\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}            \n",
    "        if self.use_collator:\n",
    "            item = self.collator([item])\n",
    "            item = {key: val[0] for key, val in item.items()}\n",
    "        item['h_labels']= self.labels[idx]\n",
    "        item['o_labels']= self.org_labels[idx]\n",
    "        \n",
    "        POS=[]\n",
    "        NEG=[]\n",
    "        \n",
    "        #------POSITIVE LINKS-----\n",
    "        p_labels=(self.labels[idx]==1).nonzero().flatten()        \n",
    "        for c in p_labels: POS.append(c.item())\n",
    "        \n",
    "        #------NEG LINKS-----\n",
    "        n_labels=(self.labels[idx]==0).nonzero().flatten()    \n",
    "        limit=min(len(n_labels),self.k_neg_link)\n",
    "        indexs=np.random.choice(len(n_labels),limit, replace=False)\n",
    "        n_labels=n_labels[indexs]            \n",
    "        for c in n_labels: NEG.append(c.item())\n",
    "        \n",
    "        NEG=(NEG*int(self.k_neg_link/len(NEG)+1))[:self.k_neg_link]\n",
    "        POS=(POS*int(self.k_neg_link/len(POS)+1))[:self.k_neg_link]\n",
    "        \n",
    "        item['pos']=torch.tensor(POS, dtype=torch.long)\n",
    "        item['neg']=torch.tensor(NEG, dtype=torch.long)\n",
    "        \n",
    "        item['pos_label']=torch.ones(len(POS), dtype=torch.long)\n",
    "        item['neg_label']=torch.zeros(len(NEG), dtype=torch.long)\n",
    "             \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing CVE dataset\n",
    "```\n",
    "A=AutoTokenizer\n",
    "berttokenizer=A.from_pretrained('bert-base-uncased')\n",
    "datacollator=DataCollatorForLanguageModeling(tokenizer=berttokenizer,mlm_probability=0.15, mlm=True)\n",
    "\n",
    "data, sentences, labels = getDummyDataset()\n",
    "if type(labels)!=torch.Tensor:\n",
    "    labels=torch.tensor(labels,dtype=torch.long)\n",
    "else:\n",
    "    labels=labels.type(torch.LongTensor)\n",
    "    \n",
    "train_encodings = berttokenizer(sentences, truncation=True, padding=True)\n",
    "dataset = CDataset(train_encodings, labels, labels, datacollator, data)\n",
    "\n",
    "dataset[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CWE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, org_labels, collator, data, use_collator=True):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.org_labels = org_labels\n",
    "        self.collator = collator\n",
    "        self.data = data\n",
    "        self.use_collator=use_collator\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}            \n",
    "        if self.use_collator:\n",
    "            item = self.collator([item])\n",
    "            item = {key: val[0] for key, val in item.items()}\n",
    "        item['h_labels']= self.labels[idx]\n",
    "        item['o_labels']= self.org_labels[idx]\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing CWE Dataset\n",
    "```\n",
    "A=AutoTokenizer\n",
    "berttokenizer=A.from_pretrained('bert-base-uncased')\n",
    "datacollator=DataCollatorForLanguageModeling(tokenizer=berttokenizer,mlm_probability=0.15, mlm=True)\n",
    "\n",
    "data, sentences, labels = getDummyDataset()\n",
    "if type(labels)!=torch.Tensor:\n",
    "    labels=torch.tensor(labels,dtype=torch.long)\n",
    "else:\n",
    "    labels=labels.type(torch.LongTensor)\n",
    "    \n",
    "    \n",
    "class_mask=(data.class_mask == True).nonzero().flatten().numpy()\n",
    "class_encodings = berttokenizer([sentences[i] for i in class_mask], truncation=True, padding=True)\n",
    "\n",
    "dataset = CDataset(class_encodings, labels, labels, datacollator, data)\n",
    "\n",
    "dataset[3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing(pl.LightningDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.save_hyperparameters()\n",
    "        if isinstance(args, tuple): args = args[0]\n",
    "        self.hparams = args\n",
    "        self.batch_size=self.hparams.batch_size        \n",
    "\n",
    "        print(f'PRETRAINED:{self.hparams.pretrained}')\n",
    "\n",
    "        A = AutoTokenizer\n",
    "        self.tokenizer = A.from_pretrained(self.hparams.pretrained, use_fast=True)\n",
    "        print('Tokenizer:', type(self.tokenizer))\n",
    "        \n",
    "        self.datacollator=DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm_probability=0.15)\n",
    "    \n",
    "    \n",
    "    def get_cwe_level(self):\n",
    "        self.levels={}\n",
    "        for key,values in self.data.depth.items():\n",
    "            if type(values)==int:\n",
    "                values=[values]\n",
    "\n",
    "            for val in values:\n",
    "                if val in self.levels:\n",
    "                    if(key not in self.levels[val]):self.levels[val].append(key)\n",
    "                else:\n",
    "                    self.levels[val]=[key]\n",
    "\n",
    "#         for i,j in self.levels.items():\n",
    "#             print(i,\"->\",len(j),':',j)\n",
    "            \n",
    "    \n",
    "    def indexsToUpdate(self,parentid,indexs):\n",
    "        if parentid==-1:\n",
    "            return\n",
    "\n",
    "        if(parentid not in indexs):\n",
    "            indexs.append(parentid)\n",
    "\n",
    "        parents=self.data.child_parent[parentid]\n",
    "\n",
    "        for parent in parents:\n",
    "                self.indexsToUpdate(parent,indexs)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def updates_label_by_hierarchy(self,labels):\n",
    "        all_mask=self.data.train_mask|self.data.val_mask|self.data.test_mask|self.data.class_mask\n",
    "        labeled_indexs= (all_mask == True).nonzero().flatten().numpy()\n",
    "\n",
    "        for i in labeled_indexs:    \n",
    "            row_labels=(labels[i]==1).nonzero().flatten().numpy()    \n",
    "\n",
    "            #print(row_labels,\"->\",end='')\n",
    "\n",
    "            indexs=[]\n",
    "            for r_label in row_labels:\n",
    "                parents=self.data.child_parent[r_label]\n",
    "\n",
    "                for parent in parents:\n",
    "                    self.indexsToUpdate(parent,indexs)\n",
    "\n",
    "            #print(indexs)\n",
    "\n",
    "            labels[i][indexs]=1\n",
    "\n",
    "        return labels\n",
    "               \n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        MAX_TEXT_LENGTH=512\n",
    "    \n",
    "#         CVE dataset\n",
    "#         ------------------------------------        \n",
    "        data, df_CVE, df_CWE=None,None,None\n",
    "    \n",
    "        if self.hparams.rand_dataset=='dummy':            \n",
    "            #------------------------------------\n",
    "            data, sentences, labels = getDummyDataset()        \n",
    "            #------------------------------------\n",
    "        else:        \n",
    "            if self.hparams.rand_dataset=='temporal':\n",
    "                print(\"Temporal Partition:--\")\n",
    "                data, df_CVE, df_CWE = getDataset(DATASET_LOAD_DIR)            \n",
    "            else:\n",
    "                print(\"Random Partition:--\")\n",
    "                data, df_CVE, df_CWE = getRandomDataset(DATASET_LOAD_DIR, 0.70, 0.10)\n",
    "\n",
    "            sentences=df_CVE['CVE Description'].apply(lambda x: x[:MAX_TEXT_LENGTH])\n",
    "            labels=data.y\n",
    "            CWE_IDS_USED=df_CWE['Name'].tolist()\n",
    "            INDEX_TO_CWE_MAP=dict(zip(list(range(len(CWE_IDS_USED))),CWE_IDS_USED))\n",
    "            CWE_TO_INDEX_MAP=dict(zip(CWE_IDS_USED,list(range(len(CWE_IDS_USED)))))\n",
    "            sentences=sentences.tolist()\n",
    "        \n",
    "#         #------------------------------------        \n",
    "#         #CVE dummy dataset\n",
    "#         #------------------------------------\n",
    "#         data, sentences, labels = getDummyDataset()        \n",
    "#         #------------------------------------\n",
    "\n",
    "        \n",
    "        \n",
    "        if type(labels)!=torch.Tensor:\n",
    "            labels=torch.tensor(labels,dtype=torch.long)\n",
    "        else:\n",
    "            labels=labels.type(torch.LongTensor)\n",
    "        self.data=data        \n",
    "        org_labels=np.copy(labels) ###keep the copy of orgiginal labels\n",
    "        \n",
    "        self.get_cwe_level()        \n",
    "        labels=self.updates_label_by_hierarchy(labels)\n",
    "\n",
    "        \n",
    "        self.NUM_CLASSES=len(data.y[0])\n",
    "        \n",
    "        train_mask= (data.train_mask == True).nonzero().flatten().numpy()\n",
    "        val_mask= (data.val_mask == True).nonzero().flatten().numpy()\n",
    "        test_mask= (data.test_mask == True).nonzero().flatten().numpy()\n",
    "        class_mask=(data.class_mask == True).nonzero().flatten().numpy()\n",
    "        \n",
    "        print(\"Train size:\", len(train_mask))\n",
    "        print(\"Val size:\", len(val_mask))\n",
    "        print(\"Test size:\", len(test_mask))\n",
    "        print(\"Class size:\",len(class_mask))\n",
    "\n",
    "        \n",
    "        train_encodings = self.tokenizer([sentences[i] for i in train_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)        \n",
    "        \n",
    "        self.dataset = CDataset(\n",
    "            train_encodings, \n",
    "            labels[data.train_mask], \n",
    "            org_labels[data.train_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS\n",
    "        )\n",
    "        self.datasetNC = CDataset(\n",
    "            train_encodings, \n",
    "            labels[data.train_mask], \n",
    "            org_labels[data.train_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS,\n",
    "            use_collator=False\n",
    "        )        \n",
    "                \n",
    "        val_encodings = self.tokenizer([sentences[i] for i in val_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)\n",
    "        self.val_dataset=CDataset(\n",
    "            val_encodings, \n",
    "            labels[data.val_mask],\n",
    "            org_labels[data.val_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS\n",
    "        )\n",
    "        self.val_datasetNC=CDataset(\n",
    "            val_encodings, \n",
    "            labels[data.val_mask],\n",
    "            org_labels[data.val_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS,\n",
    "            use_collator=False\n",
    "        )\n",
    "        \n",
    "        test_encodings = self.tokenizer([sentences[i] for i in test_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)\n",
    "        self.test_dataset=CDataset(\n",
    "            test_encodings,\n",
    "            labels[data.test_mask],\n",
    "            org_labels[data.test_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS\n",
    "        )\n",
    "        self.test_datasetNC=CDataset(\n",
    "            test_encodings,\n",
    "            labels[data.test_mask],\n",
    "            org_labels[data.test_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS,\n",
    "            use_collator=False\n",
    "        )\n",
    "        \n",
    "        class_encodings = self.tokenizer([sentences[i] for i in class_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)        \n",
    "        self.class_dataset = DDataset(\n",
    "            class_encodings, \n",
    "            labels[data.class_mask], \n",
    "            org_labels[data.class_mask],\n",
    "            self.datacollator,\n",
    "            data\n",
    "        )\n",
    "        self.class_datasetNC = DDataset(\n",
    "            class_encodings, \n",
    "            labels[data.class_mask], \n",
    "            org_labels[data.class_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            use_collator=False\n",
    "        )\n",
    "    \n",
    "    def class_dataloader(self, use_collator=True):\n",
    "        dataset=None\n",
    "        if use_collator:\n",
    "            dataset=self.class_dataset\n",
    "        else:\n",
    "            dataset=self.class_datasetNC\n",
    "            \n",
    "        class_sampler = SequentialSampler(dataset)\n",
    "        \n",
    "        loader_cwe = DataLoader(\n",
    "            dataset,\n",
    "            sampler=class_sampler, \n",
    "            batch_size=len(dataset),\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size, NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cwe\n",
    "    \n",
    "    def train_dataloader(self,use_collator=True):\n",
    "        \n",
    "        dataset=None\n",
    "        if use_collator:\n",
    "            dataset=self.dataset\n",
    "        else:\n",
    "            dataset=self.datasetNC\n",
    "        \n",
    "        train_sampler = RandomSampler(dataset)        \n",
    "        loader_cve = DataLoader(\n",
    "            dataset,\n",
    "            sampler=train_sampler, \n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size,NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cve\n",
    "    \n",
    "    def val_dataloader(self, use_collator=True):\n",
    "        dataset=None\n",
    "        \n",
    "        if use_collator:\n",
    "            dataset=self.val_dataset\n",
    "        else:\n",
    "            dataset=self.val_datasetNC\n",
    "    \n",
    "        val_sampler = SequentialSampler(dataset)\n",
    "        \n",
    "        loader_cve=DataLoader(\n",
    "            dataset,\n",
    "            sampler=val_sampler, \n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size,NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cve\n",
    "    \n",
    "    def test_dataloader(self,use_collator=True):\n",
    "        dataset=None        \n",
    "        if use_collator:\n",
    "            dataset=self.test_dataset\n",
    "        else:\n",
    "            dataset=self.test_datasetNC\n",
    "\n",
    "        test_sampler = SequentialSampler(dataset)        \n",
    "        loader_cve=DataLoader(\n",
    "            dataset,\n",
    "            sampler=test_sampler, \n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size, NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cve\n",
    "\n",
    "\n",
    "# #----------------------------\n",
    "# import argparse\n",
    "# from argparse import ArgumentParser\n",
    "\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('--pretrained', type=str, default=\"bert-base-uncased\")\n",
    "# parser.add_argument('--batch_size', type=int, default=32)\n",
    "# parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "# parser = pl.Trainer.add_argparse_args(parser)\n",
    "# args = parser.parse_args()\n",
    "# dataProcessor = DataProcessing(args)\n",
    "# dataProcessor.setup()\n",
    "# #----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Configuration to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--pretrained', type=str, default=\"bert-base-uncased\")\n",
    "    #parser.add_argument('--pretrained', type=str, default=\"roberta-base\")\n",
    "    #parser.add_argument('--pretrained', type=str, default=\"distilbert-base-uncased\")    \n",
    "    parser.add_argument('--epochs', type=int, default=25)\n",
    "    parser.add_argument('--nr_frozen_epochs', type=int, default=5)\n",
    "    #parser.add_argument('--training_portion', type=float, default=0.9)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--auto_batch', type=int, default=-1)\n",
    "    parser.add_argument('--learning_rate', type=float, default=2e-5)\n",
    "    parser.add_argument('--frac', type=float, default=1)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--nodes', type=int, default=1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--refresh_rate', type=int, default=100)\n",
    "    parser.add_argument('--checkpointing', type=str, default='True', choices=['True','False'])\n",
    "    parser.add_argument('--use_rd', type=str, default='False', choices=['True','False'])\n",
    "    parser.add_argument('--lm_lambda', type=float, default=0.1)\n",
    "    parser.add_argument('--rand_dataset', type=str, default=\"dummy\", choices=['temporal','random','dummy'])\n",
    "    parser.add_argument('--use_pretrained', type=str, default='True', choices=['True','False'])\n",
    "    parser.add_argument('--neg_link', type=int, default=120)\n",
    "    parser.add_argument('--check', type=bool, default=False)\n",
    "    parser.add_argument('--performance_mode', type=str, default='False', choices=['True','False'])\n",
    "    parser.add_argument('--freeze', type=str, default='True')\n",
    "    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "\n",
    "    # parser = Model.add_model_specific_args(parser) parser = Data.add_model_specific_args(parser)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    global K_NEG_LINKS\n",
    "    K_NEG_LINKS=args.neg_link    \n",
    "    print(\"-\"*50)\n",
    "    print(\"POS/NEG Links: \",K_NEG_LINKS)\n",
    "    print(\"BATCH SIZE: \", args.batch_size)\n",
    "    \n",
    "    \n",
    "    # start : get training steps\n",
    "    dataProcessor = DataProcessing(args)\n",
    "    dataProcessor.setup()\n",
    "    args.NUM_CLASSES=dataProcessor.NUM_CLASSES\n",
    "    args.MODEL_NAME=\"CBERT-LINK-\"+args.pretrained+'-'+args.parallel_mode\n",
    "    args.MODEL_DIR_FILE=MODEL_SAVE_DIR+args.MODEL_NAME\n",
    "    #args.PRE_TRAINED_MODEL=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'-'+args.parallel_mode+'.ckpt'\n",
    "    args.PRE_TRAINED_MODEL=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'.ckpt'\n",
    "    \n",
    "    args.num_training_steps = len(dataProcessor.train_dataloader())*args.epochs\n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    gpus=-1\n",
    "    if NUM_GPUS>0:\n",
    "        gpus=args.num_gpus\n",
    "    else:\n",
    "        gpus=None\n",
    "        args.parallel_mode=None\n",
    "    \n",
    "    print(\"USING GPUS:\", gpus)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='loss_epoch',\n",
    "        dirpath=MODEL_SAVE_DIR,\n",
    "        filename='{epoch:02d}-{loss:.4f}',\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "        save_weights_only=True,\n",
    "        prefix=\"CBERT-LINK\"+args.pretrained+'-'+str(args.parallel_mode),\n",
    "        save_last=True,\n",
    "    )\n",
    "    \n",
    "    if args.check==False:\n",
    "        args.checkpoint_callback = False\n",
    "    elif args.parallel_mode=='dp':\n",
    "        args.callbacks=[checkpoint_callback]        \n",
    "    else:\n",
    "        args.checkpoint_callback = False\n",
    "    \n",
    "    trainer = pl.Trainer.from_argparse_args(args, \n",
    "                                            gpus=gpus,\n",
    "                                            num_nodes=args.nodes, \n",
    "                                            accelerator=args.parallel_mode,\n",
    "                                            max_epochs=args.epochs, \n",
    "                                            gradient_clip_val=1.0,                                            \n",
    "                                            logger=False,\n",
    "                                            progress_bar_refresh_rate=args.refresh_rate,\n",
    "                                            profiler=False, #'simple',\n",
    "                                            default_root_dir=MODEL_SAVE_DIR,                                            \n",
    "                                            deterministic=True,\n",
    "                                           )\n",
    "    \n",
    "    return trainer, dataProcessor, args, dict_args\n",
    "\n",
    "#trainer, dataProcessor, args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForMaskedLM\n",
    "\n",
    "https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForSequenceClassification    \n",
    "\n",
    "https://huggingface.co/transformers/_modules/transformers/models/roberta/modeling_roberta.html#RobertaForMaskedLM\n",
    "\n",
    "https://huggingface.co/transformers/_modules/transformers/models/distilbert/modeling_distilbert.html#DistilBertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import gelu\n",
    "\n",
    "class BaseModelDistillBert(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self._frozen = False\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,                                            \n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "        self.config=config\n",
    "        \n",
    "        A = AutoModel \n",
    "        self.base_model = A.from_pretrained(self.hparams.pretrained, config=config)                \n",
    "                \n",
    "        self.vocab_transform = nn.Linear(config.dim, config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(config.dim, config.vocab_size)\n",
    "        \n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "        \n",
    "        self.CELoss = CrossEntropyLoss()\n",
    "\n",
    "        print('Base: ', type(self.base_model))\n",
    "    \n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        outputs = self.base_model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )        \n",
    "        \n",
    "        hidden_states = outputs[0]  # (bs, seq_length, dim)\n",
    "        \n",
    "        masked_lm_loss=None\n",
    "        if 'labels' in batch:        \n",
    "            labels=batch['labels']      \n",
    "            \n",
    "            prediction_logits = self.vocab_transform(hidden_states)  # (bs, seq_length, dim)\n",
    "            prediction_logits = gelu(prediction_logits)  # (bs, seq_length, dim)\n",
    "            prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "            prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "\n",
    "            masked_lm_loss = self.CELoss(prediction_logits.view(-1, prediction_logits.size(-1)), labels.view(-1))\n",
    "            \n",
    "            del labels, prediction_logits\n",
    "        \n",
    "        \n",
    "        pooled_output = hidden_states[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled = self.dropout(pooled_output)  # (bs, dim)\n",
    "\n",
    "        del batch\n",
    "        del outputs\n",
    "        \n",
    "        if masked_lm_loss is not None: \n",
    "            masked_lm_loss = masked_lm_loss.view(1)\n",
    "        \n",
    "        return (masked_lm_loss, pooled)\n",
    "        \n",
    "    \n",
    "# #----------------------------\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('--pretrained', type=str, default=\"distilbert-base-uncased\")\n",
    "# parser.add_argument('--batch_size', type=int, default=32)\n",
    "# parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "# args = parser.parse_args()\n",
    "# dict_args = vars(args)\n",
    "# base_model=BaseModelDistillBert(**dict_args)\n",
    "# print(base_model)\n",
    "# #----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self._frozen = False\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,                                            \n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "        self.config=config\n",
    "        \n",
    "        A = AutoModel #AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "        self.base_model = A.from_pretrained(self.hparams.pretrained, config=config)                \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        if self.hparams.pretrained in ['bert-base-uncased','bert-large-uncased']: \n",
    "            self.lm_cls = BertOnlyMLMHead(config)\n",
    "                    \n",
    "        elif self.hparams.pretrained in ['roberta-base','roberta-large']:\n",
    "            self.lm_cls = RobertaLMHead(config)\n",
    "                    \n",
    "        print(\"LM: \",type(self.lm_cls))\n",
    "        \n",
    "        self.CELoss = CrossEntropyLoss()\n",
    "\n",
    "        print('Base: ', type(self.base_model))\n",
    "        \n",
    "        print(\"Freezing Layers:-\")\n",
    "        if self.hparams.freeze=='True':                \n",
    "            if self.hparams.pretrained in ['bert-base-uncased','roberta-base']:\n",
    "                self.freeze('layer.9')\n",
    "            elif self.hparams.pretrained in ['bert-large-uncased', 'roberta-large']:\n",
    "                self.freeze('layer.21')            \n",
    "            else:\n",
    "                print(\"Nothing Frozen\")\n",
    "        elif self.hparams.freeze=='False':\n",
    "            print(\"Nothing Frozen\")\n",
    "        \n",
    "        else:\n",
    "            self.freeze(self.hparams.freeze)\n",
    "                        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        outputs = self.base_model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )        \n",
    "        \n",
    "        masked_lm_loss=None\n",
    "        if 'labels' in batch:        \n",
    "            labels=batch['labels']      \n",
    "            prediction_scores = self.lm_cls(outputs.last_hidden_state)            \n",
    "            masked_lm_loss = self.CELoss(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            del labels, prediction_scores\n",
    "        \n",
    "        pooled=outputs.pooler_output        \n",
    "        pooled=self.dropout(pooled)\n",
    "        \n",
    "        del batch\n",
    "        del outputs\n",
    "        \n",
    "        if masked_lm_loss is not None: \n",
    "            masked_lm_loss = masked_lm_loss.view(1)\n",
    "        \n",
    "        return (masked_lm_loss, pooled)\n",
    "    \n",
    "    def freeze(self,layername) -> None:        \n",
    "        for name, param in self.base_model.named_parameters():            \n",
    "#             print(name)            \n",
    "            if layername in name:\n",
    "                print(\"Froze upto: \", name)\n",
    "                break\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self._frozen = True\n",
    "        \n",
    "    \n",
    "# #----------------------------\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('--pretrained', type=str, default=\"roberta-base\")\n",
    "# parser.add_argument('--batch_size', type=int, default=32)\n",
    "# parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "# args = parser.parse_args()\n",
    "# dict_args = vars(args)\n",
    "# base_model=BaseModel(**dict_args)\n",
    "# print(base_model)\n",
    "# #----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictionModel(pl.LightningModule):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        #self.lc_1 = nn.Linear(config.hidden_size*2,2)\n",
    "        #self.lc_1 = nn.Linear(config.hidden_size,2)\n",
    "        \n",
    "        self.lc_1 = nn.Linear(config.hidden_size*2, config.hidden_size)\n",
    "        self.lc_2 = nn.Linear(config.hidden_size,2)\n",
    "        \n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.CELoss = CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, CVE_vectors, CWE_vectors, true_links=None):\n",
    "        logits = self.lc_1(torch.cat((torch.abs(CVE_vectors-CWE_vectors),CVE_vectors*CWE_vectors), 1))\n",
    "        #logits = self.lc_1(CVE_vectors*CWE_vectors)\n",
    "        logits = self.lc_2(self.tanh(logits))\n",
    "        \n",
    "        loss=None\n",
    "        if true_links is not None:\n",
    "            loss=self.CELoss(logits,true_links)     \n",
    "            \n",
    "        if loss is not None: \n",
    "            loss = loss.view(1)\n",
    "            \n",
    "        return (loss, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        \n",
    "        if self.hparams.pretrained == \"distilbert-base-uncased\":\n",
    "            self.base_model=BaseModelDistillBert(*args, **kwargs)\n",
    "        else:        \n",
    "            self.base_model=BaseModel(*args, **kwargs)\n",
    "        self.link_model=LinkPredictionModel(self.base_model.config, *args, **kwargs)\n",
    "        \n",
    "        if NUM_GPUS > 1: \n",
    "            ids=None\n",
    "            if self.hparams.num_gpus!=-1:\n",
    "                ids=list(range(self.hparams.num_gpus))\n",
    "                \n",
    "            self.base_model = nn.DataParallel(self.base_model, device_ids=ids)\n",
    "            self.link_model = nn.DataParallel(self.link_model, device_ids=ids)\n",
    "        \n",
    "\n",
    "    def forward(self, batch, CWE_pooled):                \n",
    "        lm_loss, CVE_pooled=model.base_model(batch)\n",
    "        \n",
    "        CVE_vectors=CVE_pooled[batch['CVE_index']]\n",
    "        CWE_vectors=CWE_pooled[batch['CWE_index']]\n",
    "        true_links=batch['true_labels']\n",
    "    \n",
    "        (loss, logits)=self.link_model(CVE_vectors,CWE_vectors, true_links)        \n",
    "\n",
    "        del CVE_vectors, CWE_vectors, batch\n",
    "        \n",
    "        loss=loss.mean()\n",
    "        \n",
    "        if lm_loss is not None:\n",
    "            loss+= ((self.hparams.lm_lambda)*lm_loss.mean())\n",
    "\n",
    "        return (loss, logits, true_links)\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        optimizer_grouped_parameters = [{\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.01\n",
    "        }, {\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.0\n",
    "        }]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.learning_rate,\n",
    "                          eps=1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                          )\n",
    "\n",
    "        \n",
    "        # We also use a scheduler that is supplied by transformers.\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0, # Default value in run_glue.py\n",
    "            num_training_steps=self.hparams.num_training_steps)\n",
    "\n",
    "        return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printModelParams(model):\n",
    "    print (model)\n",
    "    # Get all of the model's parameters as a list of tuples.\n",
    "    params = list(model.named_parameters())\n",
    "    print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "    for p in params[-5:]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "        \n",
    "def print_model_value(model):\n",
    "    params = list(model.named_parameters())\n",
    "    print (params[-1][0],params[-1][1][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "Two key aspects:\n",
    "\n",
    "- pytorch lightning can add arguments to the parser automatically\n",
    "- you can manually add your own specific arguments.\n",
    "\n",
    "- there is a little more code than seems necessary, because of a particular argument the scheduler\n",
    "  needs. There is currently an open issue on this complication\n",
    "  https://github.com/PyTorchLightning/pytorch-lightning/issues/1038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "POS/NEG Links:  120\n",
      "BATCH SIZE:  32\n",
      "PRETRAINED:bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/das90/.conda/envs/cent7/5.3.1-py37/cs690_37/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: Checkpoint directory ./Results/NVD/Model/ exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "Train size: 8\n",
      "Val size: 13\n",
      "Test size: 13\n",
      "Class size: 4\n",
      "USING GPUS: -1\n",
      "--------------------------------------------------\n",
      "LM:  <class 'transformers.models.bert.modeling_bert.BertOnlyMLMHead'>\n",
      "Base:  <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "Freezing Layers:-\n",
      "Froze upto:  encoder.layer.9.attention.self.query.weight\n"
     ]
    }
   ],
   "source": [
    "trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "log_results(args.MODEL_DIR_FILE+'_log.txt',dict_args,append=False)\n",
    "\n",
    "model = Model(**dict_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pretrained Model:  ./Results/NVD/Model/CBERT-bert-base-uncased.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  <class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n",
      "Matched:  198\n"
     ]
    }
   ],
   "source": [
    "class PretrainedModel(pl.LightningModule):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,\n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "        A = AutoModelForMaskedLM\n",
    "        self.model = A.from_pretrained(self.hparams.pretrained, config=config)\n",
    "        print('Model: ', type(self.model))\n",
    "\n",
    "if args.use_pretrained=='True':\n",
    "    print('Loading Pretrained Model: ',args.PRE_TRAINED_MODEL)    \n",
    "    if os.path.exists(args.PRE_TRAINED_MODEL): \n",
    "            \n",
    "        pretrainedModel=PretrainedModel(**dict_args)\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            if args.pretrained in ['bert-base-uncased','bert-large-uncased']:        \n",
    "\n",
    "                checkpoint = torch.load(args.PRE_TRAINED_MODEL, map_location=lambda storage, loc: storage)\n",
    "\n",
    "                if NUM_GPUS>1: \n",
    "                    model_dict = (model.base_model.module.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.bert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.module.base_model).load_state_dict(model_dict)\n",
    "                else:\n",
    "                    model_dict = (model.base_model.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.bert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.base_model).load_state_dict(model_dict)\n",
    "\n",
    "            elif args.pretrained in ['roberta-base','roberta-large']:\n",
    "\n",
    "                checkpoint = torch.load(args.PRE_TRAINED_MODEL, map_location=lambda storage, loc: storage)\n",
    "\n",
    "                if NUM_GPUS>1: \n",
    "                    model_dict = (model.base_model.module.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.roberta).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.module.base_model).load_state_dict(model_dict)\n",
    "                else:\n",
    "                    model_dict = (model.base_model.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.roberta).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.base_model).load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "            elif args.pretrained=='distilbert-base-uncased':\n",
    "\n",
    "                if NUM_GPUS>1: \n",
    "                    model_dict = (model.base_model.module.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.distilbert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.module.base_model).load_state_dict(model_dict)\n",
    "                else:\n",
    "                    model_dict = (model.base_model.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.distilbert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.base_model).load_state_dict(model_dict)\n",
    "        except:\n",
    "            print(\"Pytorch version missmatch between saved and new model\")\n",
    "\n",
    "    else:\n",
    "        print(\"File not found will continue with original model\")\n",
    "else:\n",
    "    print(\"Use default model as base...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depolying model to  cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Depolying model to \",device)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=dataProcessor.train_dataloader()    \n",
    "val_dataloader=dataProcessor.val_dataloader()\n",
    "test_dataloader=dataProcessor.test_dataloader()\n",
    "class_dataloader=dataProcessor.class_dataloader()\n",
    "\n",
    "train_dataloaderNC=dataProcessor.train_dataloader(use_collator=False)    \n",
    "val_dataloaderNC=dataProcessor.val_dataloader(use_collator=False)\n",
    "test_dataloaderNC=dataProcessor.test_dataloader(use_collator=False)\n",
    "class_dataloaderNC=dataProcessor.class_dataloader(use_collator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_dataloader=iter(class_dataloaderNC)\n",
    "# next(iter_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def th_link_acc(preds,labels,th=0.50):\n",
    "    pred=(preds >= th).astype(int)    \n",
    "    return np.sum(pred == labels) / len(labels)\n",
    "\n",
    "def th_link_f1_score(logits,y_true,th=0.50):\n",
    "    logits=(torch.nn.functional.softmax(logits,dim=1))[:1]\n",
    "    y_pred=(logits >= th).astype(int)    \n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "def link_f1_score(logits,y_true):\n",
    "#     print(logits)\n",
    "#     logits=(torch.nn.functional.softmax(logits,dim=1))\n",
    "    y_pred=np.argmax(logits,axis=1)\n",
    "#     print(y_pred)\n",
    "#     print(y_true)\n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "#     f1_score(y_true, y_pred, average='micro')\n",
    "#     f1_score(y_true, y_pred, average='weighted')    \n",
    "#     f1_score(y_true, y_pred, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in dataProcessor.levels.items():\n",
    "#     print(i,\"->\",len(j),':',j)\n",
    "levels=dataProcessor.levels\n",
    "data=dataProcessor.data\n",
    "\n",
    "def getHierIndexs(child, preds,k2, indexs, rec=-1):    \n",
    "    if child in data.parent_child:\n",
    "        children=data.parent_child[child]           \n",
    "        child_preds=preds[children]\n",
    "\n",
    "        k_child=min(len(children),k2)\n",
    "        child_indexs=np.argpartition(child_preds, -k_child)[-k_child:]                \n",
    "        child_index_map=dict(zip(range(len(children)),children))\n",
    "        \n",
    "        grandchildren=[child_index_map[ix] for ix in child_indexs]\n",
    "        \n",
    "        indexs.extend(grandchildren)\n",
    "    \n",
    "        if(rec==-1):\n",
    "            for grandchild in grandchildren:\n",
    "                getHierIndexs(grandchild,preds,k2, indexs)\n",
    "    \n",
    "\n",
    "def getPrediction(firstpredictions,preds,k1,k2):\n",
    "    \n",
    "    results=[]\n",
    "    \n",
    "    for child in firstpredictions:\n",
    "\n",
    "        childrens=[]\n",
    "        getHierIndexs(child, preds, k1, childrens, rec=1)\n",
    "        \n",
    "        results.append(child)\n",
    "        results.extend(childrens)\n",
    "\n",
    "        for grandchild in childrens:\n",
    "            grandchildren=[]\n",
    "            getHierIndexs(grandchild,preds,k2,grandchildren)\n",
    "            results.extend(grandchildren)\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n",
    "def top_k_accuracy(preds, true_labels, k0=1,k1=1,k2=1):\n",
    "    n0_level=levels[0]\n",
    "    n0_level_preds=preds[:,n0_level]\n",
    "    index_map=dict(zip(range(len(n0_level)),n0_level))\n",
    "    \n",
    "    k0=min(len(n0_level),k0)\n",
    "    top_k0_indexs= np.argpartition(n0_level_preds, -k0)[:,-k0:]\n",
    "    org_k0_indexs= [[index_map[j] for j in i] for i in top_k0_indexs]\n",
    "    \n",
    "    #preds_k=Parallel(n_jobs=num_processors)(delayed(getPrediction)(org_k0_indexs[i],preds[i,:],k1,k2) for i in range(len(org_k0_indexs)))    \n",
    "    preds_k=[getPrediction(org_k0_indexs[i],preds[i,:],k1,k2) for i in range(len(org_k0_indexs))]    \n",
    "    clusters=[np.where(t_label == 1)[0] for t_label in true_labels]\n",
    "    \n",
    "    corrects=[bool(set(clusters[i]) & set(preds_k[i])) for i in range(len(preds_k))]\n",
    "    \n",
    "    return sum(corrects)/len(corrects)\n",
    "\n",
    "# preds=np.array([\n",
    "#     [0.1,0.7,0.4,0.6],\n",
    "#     [0,1,0,0],\n",
    "#     [0.3,0.1,0,0.4]])\n",
    "# labels=np.array([\n",
    "#     [1,0,0,0],\n",
    "#     [0,1,0,0],\n",
    "#     [0,0,0,1]])\n",
    "\n",
    "# print(top_k_accuracy(preds,labels,k0=1,k1=1,k2=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_links(batch):    \n",
    "    POS=batch['pos']\n",
    "    NEG=batch['neg']\n",
    "    POS_label=batch['pos_label']\n",
    "    NEG_label=batch['neg_label']\n",
    "    CWE_index=torch.cat((POS.view(-1),NEG.view(-1)))    \n",
    "    CVE_index=torch.cat((torch.tensor(np.repeat(range(POS.shape[0]),K_NEG_LINKS),dtype=torch.long),\n",
    "               torch.tensor(np.repeat(range(NEG.shape[0]),K_NEG_LINKS),dtype=torch.long)))\n",
    "    \n",
    "    true_links=torch.cat((POS_label.view(-1),NEG_label.view(-1)))\n",
    "    \n",
    "    return CVE_index, CWE_index, true_links\n",
    "\n",
    "def prepare_all_links(nCVEs=2,nCWEs=3):    \n",
    "    CVE_index=torch.tensor(np.repeat(range(nCVEs),nCWEs),dtype=torch.long)\n",
    "    CWE_index=torch.tensor(np.tile(range(nCWEs), nCVEs),dtype=torch.long)    \n",
    "    return CVE_index, CWE_index\n",
    "\n",
    "# prepare_all_links()\n",
    "# iter_train_dataloader=iter(dataProcessor.train_dataloader())\n",
    "# batch=next(iter_train_dataloader)\n",
    "# print(batch)\n",
    "# prepare_links(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute accuracy hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LINK_evaluate_model(dataloader, c_dataloader):\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    eval_accuracies=np.zeros(len(TOP_K0))\n",
    "    \n",
    "    num_batches=len(dataloader)\n",
    "    total_loss = 0\n",
    "    loss_value = -1\n",
    "    nb_steps=0\n",
    "    nb_links=0\n",
    "    step_time=0\n",
    "    total_step_time=0\n",
    "    current_time=0\n",
    "    epoch_start=time.time()\n",
    "    total_acc=0\n",
    "    step_acc=0\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        class_batch=next(iter(c_dataloader))\n",
    "        nCWEs=class_batch['o_labels'].shape[0]\n",
    "        \n",
    "        for key,value in class_batch.items(): \n",
    "            class_batch[key]=value.to(device)\n",
    "        \n",
    "        (_, CWE_pooled)=model.base_model(class_batch) #0-classlmloss, 1-classpooled \n",
    "        \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            step_start=time.time()\n",
    "            if step % args.refresh_rate == 0:            \n",
    "                print(\n",
    "                    'Batch {:}/{:} - {:0.4f} s/it, {:0.4f} s - Elapsed: {:0.4f} s'.format(\n",
    "                    step,\n",
    "                    num_batches,\n",
    "                    step_time,\n",
    "                    total_step_time,\n",
    "                    time.time()-epoch_start)\n",
    "                )\n",
    "            \n",
    "            nCVEs=batch['o_labels'].shape[0]            \n",
    "            batch['nCVEs'], batch['nCWEs']=prepare_all_links(nCVEs,nCWEs)            \n",
    "            for key,value in batch.items(): batch[key]=value.to(device)\n",
    "            \n",
    "            (_, CVE_pooled) = model.base_model(batch)\n",
    "            \n",
    "            CVE_vectors=CVE_pooled[batch['nCVEs']]\n",
    "            CWE_vectors=CWE_pooled[batch['nCWEs']]\n",
    "            \n",
    "            (_, logits)=model.link_model(CVE_vectors,CWE_vectors)\n",
    "            \n",
    "            logits=(torch.nn.functional.softmax(logits,dim=1))[:,1]\n",
    "            logits=logits.view(nCVEs,-1)\n",
    "            loggits=logits.detach().cpu().numpy()\n",
    "            true_labels=batch['o_labels'].cpu().numpy()\n",
    "            \n",
    "            step_time=time.time()-step_start\n",
    "            total_step_time+=step_time\n",
    "            \n",
    "            for k_i in range(len(TOP_K0)):\n",
    "                tmp_eval=top_k_accuracy(loggits, true_labels, TOP_K0[k_i], TOP_K1[k_i], TOP_K2[k_i])*len(logits)\n",
    "                eval_accuracies[k_i]+=tmp_eval\n",
    "                \n",
    "            nb_eval_examples+=len(logits)            \n",
    "            \n",
    "    eval_accuracies=eval_accuracies/nb_eval_examples\n",
    "        \n",
    "    print(\"-\"*25)\n",
    "    for k_i in range(len(TOP_K0)):\n",
    "        print(\" Top {0},{1},{2}... Accuracy: {3:.4f}\".format(TOP_K0[k_i],TOP_K1[k_i],TOP_K2[k_i], eval_accuracies[k_i]))\n",
    "    print(\"-\"*25)\n",
    "    \n",
    "    return eval_accuracies\n",
    "\n",
    "#LINK_evaluate_model(val_dataloaderNC, class_dataloaderNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_links(dataloader, c_dataloader):\n",
    "    num_batches=len(dataloader)\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_value = -1\n",
    "    nb_steps=0\n",
    "    nb_links=0\n",
    "    step_time=0\n",
    "    total_step_time=0\n",
    "    current_time=0\n",
    "    epoch_start=time.time()\n",
    "    total_acc=0\n",
    "    step_acc=0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        class_batch=next(iter(c_dataloader))\n",
    "        for key,value in class_batch.items(): \n",
    "            class_batch[key]=value.to(device)\n",
    "        \n",
    "        class_outputs=model.base_model(class_batch) #0-classlmloss, 1-classpooled\n",
    "        class_lm_loss=class_outputs[0]\n",
    "        \n",
    "        if class_lm_loss is not None:\n",
    "            total_loss+=(args.lm_lambda)*(class_lm_loss.mean()).item()\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            step_start=time.time()\n",
    "            if step % args.refresh_rate == 0:            \n",
    "                print(\n",
    "                    'Batch {:}/{:} - {:0.4f} s/it, {:0.4f} s - Elapsed: {:0.4f} s, loss_step {:0.4f}, loss_epoch {:0.4f} - eval_f1_step {:0.4f}, eval_f1_epoch {:0.4f}'.format(\n",
    "                    step,\n",
    "                    num_batches,\n",
    "                    step_time,\n",
    "                    total_step_time,\n",
    "                    time.time()-epoch_start, \n",
    "                    loss_value,\n",
    "                    total_loss/max(nb_steps,1),\n",
    "                    step_acc,\n",
    "                    total_acc/max(nb_links,1)))\n",
    "\n",
    "            batch['CVE_index'], batch['CWE_index'], batch['true_labels']=prepare_links(batch)       \n",
    "            for key,value in batch.items(): batch[key]=value.to(device)\n",
    "            \n",
    "            outputs = model(batch, class_outputs[1]) #0-loss, 1-logits, 2-true-links            \n",
    "            \n",
    "            loss = outputs[0].mean()\n",
    "\n",
    "            step_time=time.time()-step_start\n",
    "            total_step_time+=step_time\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            total_loss+=loss_value\n",
    "            logits=(torch.nn.functional.softmax(outputs[1].detach(),dim=1))\n",
    "            logits= logits.cpu().numpy()\n",
    "            true_links=outputs[2].detach().cpu().numpy()\n",
    "\n",
    "            nb_steps+=1\n",
    "            nb_links+=len(true_links)\n",
    "\n",
    "            step_acc=link_f1_score(logits,true_links)\n",
    "            total_acc+=step_acc*len(true_links)\n",
    "    \n",
    "    eval_loss=total_loss/nb_steps\n",
    "    eval_accuracy=total_acc/nb_links\n",
    "    \n",
    "    return eval_loss, eval_accuracy\n",
    "\n",
    "#Evaluate_links(val_dataloaderNC, class_dataloaderNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0059 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/das90/.conda/envs/cent7/5.3.1-py37/cs690_37/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6982\n",
      "Train F1-Score: 0.3333\n",
      "Train time: 0.3189 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0619 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.3750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0765 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6928\n",
      " Eval F1-Score: 0.3333\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0681 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.3077\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Saving model....acc: 0.3076923076923077\n",
      "Epoch 1/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0036 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6951\n",
      "Train F1-Score: 0.3191\n",
      "Train time: 0.2868 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0600 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.6250\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0597 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6883\n",
      " Eval F1-Score: 0.5439\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0682 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.4615\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Saving model....acc: 0.46153846153846156\n",
      "Epoch 2/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0039 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6931\n",
      "Train F1-Score: 0.4682\n",
      "Train time: 0.2998 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0589 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.6250\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0767 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6852\n",
      " Eval F1-Score: 0.4452\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0667 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.4615\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 3/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0079 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6917\n",
      "Train F1-Score: 0.6035\n",
      "Train time: 0.3052 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0588 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.6250\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0752 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6818\n",
      " Eval F1-Score: 0.3747\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0734 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.5385\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Saving model....acc: 0.5384615384615384\n",
      "Epoch 4/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0042 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6943\n",
      "Train F1-Score: 0.4386\n",
      "Train time: 0.2986 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0589 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7500\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0713 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6771\n",
      " Eval F1-Score: 0.5081\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0737 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.6923\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Saving model....acc: 0.6923076923076923\n",
      "Epoch 5/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0030 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6857\n",
      "Train F1-Score: 0.3333\n",
      "Train time: 0.2857 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0555 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0750 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6707\n",
      " Eval F1-Score: 0.6513\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0731 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Saving model....acc: 0.8461538461538461\n",
      "Epoch 6/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0030 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6856\n",
      "Train F1-Score: 0.5385\n",
      "Train time: 0.2846 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0599 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0749 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6651\n",
      " Eval F1-Score: 0.7423\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0652 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 7/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0079 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6823\n",
      "Train F1-Score: 0.5934\n",
      "Train time: 0.3070 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0610 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0753 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6588\n",
      " Eval F1-Score: 0.7182\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0738 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.9231\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Saving model....acc: 0.9230769230769231\n",
      "Epoch 8/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0030 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6705\n",
      "Train F1-Score: 0.5697\n",
      "Train time: 0.2855 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0600 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0750 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6511\n",
      " Eval F1-Score: 0.7679\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0736 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.9231\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 9/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0080 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6825\n",
      "Train F1-Score: 0.5608\n",
      "Train time: 0.3117 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0612 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0746 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average evaluation loss: 0.6430\n",
      " Eval F1-Score: 0.8074\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0739 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 10/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0078 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6582\n",
      "Train F1-Score: 0.5759\n",
      "Train time: 0.3119 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0606 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0750 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6340\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0727 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.9231\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 11/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0079 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6703\n",
      "Train F1-Score: 0.6761\n",
      "Train time: 0.2957 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0606 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0751 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6249\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0743 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.9231\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 12/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0078 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6588\n",
      "Train F1-Score: 0.7500\n",
      "Train time: 0.3104 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0622 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0681 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6155\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0737 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 13/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0079 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6336\n",
      "Train F1-Score: 0.8118\n",
      "Train time: 0.3150 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0587 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0753 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6061\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0739 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 14/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0081 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6332\n",
      "Train F1-Score: 0.7117\n",
      "Train time: 0.3091 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0598 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0754 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5968\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0666 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 15/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0080 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6640\n",
      "Train F1-Score: 0.6021\n",
      "Train time: 0.3083 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0590 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0726 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5881\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0729 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 16/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0077 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6359\n",
      "Train F1-Score: 0.7793\n",
      "Train time: 0.3098 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0599 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0748 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5802\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0734 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 17/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0080 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6516\n",
      "Train F1-Score: 0.6630\n",
      "Train time: 0.3109 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0579 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0751 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5732\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0738 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 18/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0073 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6302\n",
      "Train F1-Score: 0.6770\n",
      "Train time: 0.3086 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0575 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0746 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5671\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0727 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 19/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0076 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6062\n",
      "Train F1-Score: 0.7262\n",
      "Train time: 0.3053 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0589 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0702 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5617\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0741 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 20/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0075 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.5994\n",
      "Train F1-Score: 0.7290\n",
      "Train time: 0.3095 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0607 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0697 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5573\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.2177 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 21/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0081 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6037\n",
      "Train F1-Score: 0.7857\n",
      "Train time: 0.3133 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0588 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0750 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5537\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0716 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 22/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0078 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6259\n",
      "Train F1-Score: 0.6863\n",
      "Train time: 0.3126 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0602 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0670 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5509\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0740 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 23/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0078 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.5626\n",
      "Train F1-Score: 0.8330\n",
      "Train time: 0.3116 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0608 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0736 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5491\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0729 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 24/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0083 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.5997\n",
      "Train F1-Score: 0.7185\n",
      "Train time: 0.3093 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0584 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0772 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.5481\n",
      " Eval F1-Score: 0.8462\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0700 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Link Prediction Training complete!\n",
      "Saving Last model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0700 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average test loss: 0.5481\n",
      "Evaluate test model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0674 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_model(dataloader, c_dataloader, v_dataloader=None, t_dataloader=None):    \n",
    "    optimizer, scheduler = model.configure_optimizers()\n",
    "    num_batches=len(dataloader)\n",
    "    \n",
    "    train_losses=[]\n",
    "    train_accs=[]\n",
    "    val_losses=[]\n",
    "    val_accs=[]\n",
    "    train_class_acc=[]\n",
    "    val_class_acc=[]\n",
    "    best_val=-1\n",
    "    \n",
    "    total_train_time=0\n",
    "    \n",
    "    for epoch_i in range(args.epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        loss_value = -1\n",
    "        nb_steps=0\n",
    "        nb_links=0\n",
    "        step_time=0\n",
    "        total_step_time=0\n",
    "        current_time=0\n",
    "        epoch_start=time.time()\n",
    "        total_acc=0\n",
    "        step_acc=0\n",
    "        \n",
    "        model.train()\n",
    "        class_batch=next(iter(c_dataloader))\n",
    "        for key,value in class_batch.items(): class_batch[key]=value.to(device)\n",
    "            \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            step_start=time.time()\n",
    "            if step % args.refresh_rate == 0:            \n",
    "                print(\n",
    "                    'Epoch {:}/{:} Batch {:}/{:} - {:0.4f} s/it, {:0.4f} s - Elapsed: {:0.4f} s, loss_step {:0.4f}, loss_epoch {:0.4f} - train_f1_step {:0.4f}, train_f1_epoch {:0.4f}'.format(\n",
    "                    epoch_i,\n",
    "                    args.epochs, \n",
    "                    step,\n",
    "                    num_batches,\n",
    "                    step_time,\n",
    "                    total_step_time,\n",
    "                    time.time()-epoch_start, \n",
    "                    loss_value,\n",
    "                    total_loss/max(nb_steps,1),\n",
    "                    step_acc,\n",
    "                    total_acc/max(nb_links,1)))\n",
    "            \n",
    "            batch['CVE_index'], batch['CWE_index'], batch['true_labels']=prepare_links(batch)       \n",
    "            for key,value in batch.items(): batch[key]=value.to(device)\n",
    "                            \n",
    "            model.zero_grad()\n",
    "            class_outputs=model.base_model(class_batch) #0-classlmloss, 1-classpooled\n",
    "            class_lm_loss=class_outputs[0]  \n",
    "            \n",
    "            outputs = model(batch, class_outputs[1]) #0-loss, 1-logits, 2-true-links        \n",
    "            \n",
    "            loss = outputs[0].mean()            \n",
    "            if class_lm_loss is not None:\n",
    "                loss+= (args.lm_lambda)*class_lm_loss.mean()\n",
    "            \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "            \n",
    "            optimizer.step() \n",
    "            scheduler.step()\n",
    "            \n",
    "            step_time=time.time()-step_start\n",
    "            total_step_time+=step_time\n",
    "            \n",
    "            #stats\n",
    "            loss_value = loss.item()\n",
    "            total_loss+=loss_value\n",
    "            logits=(torch.nn.functional.softmax(outputs[1].detach(),dim=1))\n",
    "            logits= logits.cpu().numpy()\n",
    "            true_links=outputs[2].detach().cpu().numpy()\n",
    "            \n",
    "            nb_steps+=1\n",
    "            nb_links+=len(true_links)\n",
    "            \n",
    "            step_acc=link_f1_score(logits,true_links)\n",
    "            total_acc+=step_acc*len(true_links)\n",
    "            \n",
    "        total_train_time+=total_step_time\n",
    "\n",
    "        avg_train_loss=total_loss/nb_steps\n",
    "        avg_train_acc=total_acc/nb_links\n",
    "        \n",
    "        print(\"Train loss: {0:.4f}\".format(avg_train_loss))\n",
    "        print(\"Train F1-Score: {0:.4f}\".format(avg_train_acc))\n",
    "        print(\"Train time: {0:.4f} sec\".format(total_step_time))\n",
    "        \n",
    "        if args.performance_mode=='True':return\n",
    "    \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accs.append(avg_train_acc)\n",
    "        \n",
    "        print(\"Evaluate train model\")\n",
    "        train_cwe_acc=LINK_evaluate_model(dataloader,c_dataloader)\n",
    "        train_class_acc.append(train_cwe_acc[0])\n",
    "        #print(train_cwe_acc)\n",
    "        \n",
    "        if v_dataloader is not None:\n",
    "            print(\"Validation.....\")        \n",
    "            eval_loss, eval_accuracy=Evaluate_links(v_dataloader, c_dataloader)      \n",
    "        \n",
    "            print(\" Average evaluation loss: {0:.4f}\".format(eval_loss))\n",
    "            print(\" Eval F1-Score: {0:.4f}\".format(eval_accuracy))\n",
    "        \n",
    "            val_accs.append(eval_accuracy)\n",
    "            val_losses.append(eval_loss)\n",
    "         \n",
    "            print(\"Evaluate validation model\")\n",
    "            val_cwe_acc=LINK_evaluate_model(v_dataloader,c_dataloader)\n",
    "            val_class_acc.append(val_cwe_acc[0])\n",
    "            \n",
    "            if args.checkpointing=='True':\n",
    "                if(best_val is None or val_cwe_acc[0]>best_val):\n",
    "                    best_val=val_cwe_acc[0]\n",
    "                    print(\"Saving model....acc:\",best_val)\n",
    "                    torch.save(model.state_dict(), args.MODEL_DIR_FILE+'_BEST')\n",
    "\n",
    "    print(\"Link Prediction Training complete!\")\n",
    "    \n",
    "    if args.checkpointing=='True':\n",
    "        print(\"Saving Last model\")\n",
    "        torch.save(model.state_dict(), args.MODEL_DIR_FILE+'_LAST')\n",
    "    \n",
    "    if t_dataloader is not None:    \n",
    "        eval_loss, eval_accuracy=Evaluate_links(t_dataloader,c_dataloader)              \n",
    "        print(\" Average test loss: {0:.4f}\".format(eval_loss))\n",
    "\n",
    "        print(\"Evaluate test model\")\n",
    "        test_cwe_acc=LINK_evaluate_model(t_dataloader,c_dataloader)        \n",
    "        log_results(args.MODEL_DIR_FILE+'_log.txt', {'test_accuracies':test_cwe_acc})\n",
    "    \n",
    "    from ipynb.fs.full.lib.Utils import save_plot\n",
    "    save_plot(train_accs, val_accs, name=args.MODEL_DIR_FILE+'_link_acc',yname='Accuracy')\n",
    "    save_plot(train_losses, val_losses, name=args.MODEL_DIR_FILE+'_link_loss',yname='Loss')\n",
    "    save_plot(train_class_acc, val_class_acc, name=args.MODEL_DIR_FILE+'_class_acc',yname='Accuracy')\n",
    "    \n",
    "    log_results(args.MODEL_DIR_FILE+'_log.txt',\n",
    "               {'train_accs':train_accs, 'val_accs':val_accs,\n",
    "                'train_losses':train_losses, 'val_losses':val_losses,\n",
    "                'train_class_acc':train_class_acc, 'val_class_acc':val_class_acc})\n",
    "    \n",
    "    log_results(args.MODEL_DIR_FILE+'_log.txt', {\n",
    "        'total_step_time':[total_step_time],\n",
    "        'num_batches':[num_batches],\n",
    "        'epochs':[args.epochs],\n",
    "        'batch_size':[args.batch_size]})\n",
    "    \n",
    "    return\n",
    "\n",
    "if args.use_rd == 'True':\n",
    "    train_model(train_dataloader, class_dataloader, val_dataloaderNC, test_dataloaderNC)\n",
    "else:\n",
    "    train_model(train_dataloaderNC, class_dataloaderNC, val_dataloaderNC, test_dataloaderNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Saved Model:  ./Results/NVD/Model/CBERT-LINK-bert-base-uncased-dp_LAST\n",
      "Original weights: \n",
      "link_model.module.lc_2.bias tensor([-0.0135,  0.0293], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Loaded weights: \n",
      "link_model.module.lc_2.bias tensor([-0.0135,  0.0293], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0510 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0727 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      "Test F-1 Score:  0.8461538461538461\n",
      "Test Complete......\n"
     ]
    }
   ],
   "source": [
    "def test_model(t_dataloader, c_dataloader):\n",
    "    MODEL_NAME=args.MODEL_DIR_FILE+'_LAST'    \n",
    "\n",
    "    if os.path.exists(MODEL_NAME): \n",
    "        print('Loading Saved Model: ',MODEL_NAME)        \n",
    "    else: \n",
    "        print(\"File not found: \",MODEL_NAME)\n",
    "        return\n",
    "    \n",
    "    print(\"Original weights: \");print_model_value(model)\n",
    "    checkpoint = torch.load(MODEL_NAME, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    print(\"Loaded weights: \");print_model_value(model)    \n",
    "    LINK_evaluate_model(test_dataloaderNC, class_dataloaderNC)    \n",
    "    \n",
    "    eval_loss, eval_accuracy = Evaluate_links(test_dataloaderNC, class_dataloaderNC)\n",
    "    print(\"Test F-1 Score: \", eval_accuracy)\n",
    "    \n",
    "    print(\"Test Complete......\")\n",
    "    \n",
    "    return\n",
    "\n",
    "if args.checkpointing=='True':\n",
    "    test_model(test_dataloaderNC, class_dataloaderNC)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda cs690_37",
   "language": "python",
   "name": "cs690_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
